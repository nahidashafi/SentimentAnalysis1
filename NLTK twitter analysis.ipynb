{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLTK: Twitter Sentiment Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Natural Language Processing (NLP)]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "SUPERVISED CLASSIFICATION:\n",
    "Here, Iam doing supervised text classification. In supervised classification, the classifier is trained Python NLTK: Text Classification [Natural Language\n",
    "Processing (NLP)] with labeled training data. Python NLTK: Working with WordNet [Natural LanguageProcessing (NLP)]\n",
    " i will use the NLTK’s twitter_samples corpus as my labeled training data. The Python NLTK: Stemming & Lemmatization [Natural Language Processing (NLP)] twitter_samples corpus contains 2K movie reviews with sentiment polarity classification. It’s Python NLTK: Stop Words [Natural Language Processing(NLP)]compiled by Pang, Lee. \n",
    " \n",
    " \n",
    " \n",
    " Natural Language Processing (NLP): Basic Introduction to NLTK [Python].Here, I have two categories for classification. They are: positive and negative. The twitter_samples Python NLTK: Part-of-Speech (POS) Tagging [Natural\n",
    "Language Processing (NLP)]corpus already /has the tweets categorized as positive and negative. Machine Learning & Sentiment Analysis: Text Classification using Python & NLTK Recommender System using Python & Crab\n",
    "\n",
    "The twitter_samples corpus contains 3 files.\n",
    "1) negative_tweets.json: contains 5k negative tweets\n",
    "2) positive_tweets.json: contains 5k positive tweets\n",
    "3) tweets.20150430-223406.json: contains 20k positive and negative tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['negative_tweets.json', 'positive_tweets.json', 'tweets.20150430-223406.json']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import twitter_samples\n",
    "print (twitter_samples.fileids())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20000\n"
     ]
    }
   ],
   "source": [
    "pos_tweets = twitter_samples.strings('positive_tweets.json')\n",
    "print (len(pos_tweets))\n",
    "neg_tweets = twitter_samples.strings('negative_tweets.json')\n",
    "print (len(neg_tweets))\n",
    "all_tweets = twitter_samples.strings('tweets.20150430-223406.json')\n",
    "print (len(all_tweets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#FollowFriday @France_Inte @PKuchly57 @Milipol_Paris for being top engaged members in my community this week :)\n@Lamb2ja Hey James! How odd :/ Please call our Contact Centre on 02392441234 and we will be able to assist you :) Many thanks!\n@DespiteOfficial we had a listen last night :) As You Bleed is an amazing track. When are you in Scotland?!\n@97sides CONGRATS :)\nyeaaaah yippppy!!!  my accnt verified rqst has succeed got a blue tick mark on my fb profile :) in 15 days\n"
     ]
    }
   ],
   "source": [
    "for tweet in pos_tweets[:5]:\n",
    "    print (tweet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['negative_tweets.json', 'positive_tweets.json', 'tweets.20150430-223406.json']\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords_english = stopwords.words('english')\n",
    " \n",
    "from nltk.stem import PorterStemmer\n",
    "stemmer = PorterStemmer()\n",
    " \n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "import string\n",
    "import re\n",
    "from nltk.corpus import twitter_samples\n",
    "print (twitter_samples.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tokenize Tweets"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "NLTK has a TweetTokenizer module that does a good job in tokenizing (splitting text into a list of\n",
    "words) tweets.Three different parameters can be passed while calling the TweetTokenizer class. They are:\n",
    "\n",
    "preserve_case: if False then it converts tweet to lowercase and vice-versa.\n",
    "strip_handles: if True then it removes twitter handles from the tweet and vice-versa.\n",
    "reduce_len: if True then it reduces the length of words in the tweet like hurrayyyy, yipppiieeee,etc. and vice-versa.\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Cleaning Tweet\n",
    "In the tweet cleaning process, we will do the following:\n",
    "– Remove stock market tickers like $GE\n",
    "– Remove retweet text “RT”\n",
    "– Remove hyperlinks\n",
    "– Remove hashtags (only the hashtag # and not the word)\n",
    "– Remove stop words like a, and, the, is, are, etc.\n",
    "– Remove emoticons like :), :D, :(, :-), etc.\n",
    "– Remove punctuation like full-stop, comma, exclamation sign, etc.\n",
    "– Convert words to Stem/Base words using Porter Stemming Algorithm. E.g. words like ‘working’,\n",
    "‘works’, and ‘worked’ will be converted to their base/stem word “work”.\n",
    "We will define a function named clean_tweets which returns a list of cleaned (by removing the\n",
    "above-mentioned things) words for any given tweet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clean_tweets(tweet):\n",
    "    # remove stock market tickers like $GE\n",
    "    tweet = re.sub(r'\\$\\w*', '', tweet)\n",
    " \n",
    "    # remove old style retweet text \"RT\"\n",
    "    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n",
    " \n",
    "    # remove hyperlinks\n",
    "    tweet = re.sub(r'https?:\\/\\/.*[\\r\\n]*', '', tweet)\n",
    "    \n",
    "    # remove hashtags\n",
    "    # only removing the hash # sign from the word\n",
    "    tweet = re.sub(r'#', '', tweet)\n",
    " \n",
    "    # tokenize tweets\n",
    "    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)\n",
    "    tweet_tokens = tokenizer.tokenize(tweet)\n",
    " \n",
    "    tweets_clean = []    \n",
    "    for word in tweet_tokens:\n",
    "        if (word not in stopwords_english and # remove stopwords\n",
    "              \n",
    "                word not in string.punctuation): # remove punctuation\n",
    "            #tweets_clean.append(word)\n",
    "            stem_word = stemmer.stem(word) # stemming word\n",
    "            tweets_clean.append(stem_word)\n",
    " \n",
    "    return tweets_clean\n",
    " \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "@BhaktisBanter @PallaviRuhail This one is irresistible :)\n#FlipkartFashionFriday http://t.co/EbZ0L2VENM\n"
     ]
    }
   ],
   "source": [
    "print (pos_tweets[5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['one', 'irresist', ':)', 'flipkartfashionfriday']\n"
     ]
    }
   ],
   "source": [
    "print (clean_tweets(pos_tweets[5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Extraction\n",
    "We define a simple bag_of_words function that extracts unigram features from the tweets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature extractor function\n",
    "def bag_of_words(tweet):\n",
    "    words = clean_tweets(tweet)\n",
    "    words_dictionary = dict([word, True] for word in words)    \n",
    "    return words_dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_tweet = \"RT @Twitter @chapagain Hello There! Have a great day. :) #good #morning http://chapagain.com.np\"\n",
    "print (bag_of_words(custom_tweet))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5000 5000\n"
     ]
    }
   ],
   "source": [
    "# positive tweets feature set\n",
    "pos_tweets_set = []\n",
    "for tweet in pos_tweets:\n",
    "    pos_tweets_set.append((bag_of_words(tweet), 'pos'))    \n",
    "# negative tweets feature set\n",
    "neg_tweets_set = []\n",
    "for tweet in neg_tweets:\n",
    "    neg_tweets_set.append((bag_of_words(tweet), 'neg'))\n",
    "print (len(pos_tweets_set), len(neg_tweets_set))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create Train and Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 5000 positive tweets set and 5000 negative tweets set. We take 20% (i.e. 1000) of positive\n",
    "tweets and 20% (i.e. 1000) of negative tweets as the test set. The remaining negative and positive\n",
    "tweets will be taken as the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2000 8000\n"
     ]
    }
   ],
   "source": [
    "# radomize pos_reviews_set and neg_reviews_set\n",
    "# doing so will output different accuracy result everytime we run the program\n",
    "\n",
    "from random import shuffle\n",
    "shuffle(pos_tweets_set)\n",
    "shuffle(neg_tweets_set)\n",
    "  \n",
    "test_set = pos_tweets_set[:1000] + neg_tweets_set[:1000]\n",
    "train_set = pos_tweets_set[1000:] + neg_tweets_set[1000:]\n",
    "print(len(test_set),  len(train_set)) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Classifier and Calculating Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We train Naive Bayes Classifier using the training set and calculate the classification accuracy of the\n",
    "trained classifier using the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9915\nMost Informative Features\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                      :) = True              pos : neg    =   1099.0 : 1.0\n                     via = True              pos : neg    =     35.7 : 1.0\n                     sad = True              neg : pos    =     27.6 : 1.0\n                     bam = True              pos : neg    =     26.3 : 1.0\n                     x15 = True              neg : pos    =     20.3 : 1.0\n                     ugh = True              neg : pos    =     16.3 : 1.0\n                    blog = True              pos : neg    =     16.3 : 1.0\n                 perfect = True              pos : neg    =     14.3 : 1.0\n                      aw = True              neg : pos    =     13.8 : 1.0\n                    glad = True              pos : neg    =     13.0 : 1.0\nNone\n"
     ]
    }
   ],
   "source": [
    "from nltk import classify\n",
    "from nltk import NaiveBayesClassifier\n",
    " \n",
    "classifier = NaiveBayesClassifier.train(train_set)\n",
    " \n",
    "accuracy = classify.accuracy(classifier, test_set)\n",
    "print(accuracy) # Output: 0.765\n",
    " \n",
    "print (classifier.show_most_informative_features(10))    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Classifier with Custom Tweet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I provide custom tweet and check the classification output of the trained classifier. The classifier\n",
    "correctly predicts both negative and positive tweets provided."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NEGATIVE TWEET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "neg\n<ProbDist with 2 samples>\nneg\n0.9128249642254496\n0.08717503577454862\n"
     ]
    }
   ],
   "source": [
    "custom_tweet = \"I hated the film. It was a disaster. Poor direction, bad acting.\"\n",
    "custom_tweet_set = bag_of_words(custom_tweet)\n",
    "print (classifier.classify(custom_tweet_set)) # Output: neg\n",
    " # Negative tweet correctly classified as negative\n",
    "  \n",
    " # probability result\n",
    "prob_result = classifier.prob_classify(custom_tweet_set)\n",
    "print (prob_result) # Output: <ProbDist with 2 samples>\n",
    "print (prob_result.max()) # Output: neg\n",
    "print (prob_result.prob(\"neg\")) # Output: 0.941844352481\n",
    "print (prob_result.prob(\"pos\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# POSITIVE TWEET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pos\n<ProbDist with 2 samples>\npos\n0.0009002610289191033\n0.9990997389710786\n"
     ]
    }
   ],
   "source": [
    "custom_tweet = \"It was a wonderful and amazing movie. I loved it. Best direction, good acting.\"\n",
    "custom_tweet_set = bag_of_words(custom_tweet)\n",
    "  \n",
    "print (classifier.classify(custom_tweet_set)) # Output: pos\n",
    " # Positive tweet correctly classified as positive\n",
    "  \n",
    " # probability result\n",
    "prob_result = classifier.prob_classify(custom_tweet_set)\n",
    "print (prob_result)\n",
    "print (prob_result.max()) \n",
    "print (prob_result.prob(\"neg\"))\n",
    "print (prob_result.prob(\"pos\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
